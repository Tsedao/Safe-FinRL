{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import itertools\n",
    "import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "import time\n",
    "\n",
    "from sac import SAC\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from replay_buffer import ReplayMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stock_trading_env import StockTradingEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_trade_path = './train_trade_table.npy'\n",
    "val_trade_path = './val_trade_table.npy'\n",
    "train_feature_path = './train_feature_array_standard.npy'\n",
    "val_feature_path = './val_feature_array_standard.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "SEEDS = 666\n",
    "replay_size = 10000\n",
    "\n",
    "look_back = 10       # size of extracted window\n",
    "updates_per_step = 1 # model updates per simulator step (default: 1)\n",
    "num_stock = 1\n",
    "balance = 100000\n",
    "\n",
    "hidden_size = 64\n",
    "\n",
    "gamma = 0.99       # discount factor\n",
    "tau = 0.005        # target function update parameter\n",
    "lr = 0.0001\n",
    "alpha = 0.2        # weight of entropy term\n",
    "\n",
    "h_max = 1          # upper/lower bound of action\n",
    "batch_size = 12\n",
    "num_episodes = 500\n",
    "target_update_interval = 1\n",
    "updates_per_step = 1\n",
    "\n",
    "automatic_entropy_tuning = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_trade = np.load(train_trade_path)\n",
    "val_trade = np.load(val_trade_path)\n",
    "train_feature = np.load(train_feature_path)\n",
    "val_feature = np.load(val_feature_path)\n",
    "\n",
    "\n",
    "# concat the feature and the original information\n",
    "train_states = np.concatenate([train_feature,train_trade],axis=1)\n",
    "val_states = np.concatenate([val_feature,val_trade],axis=1)\n",
    "\n",
    "num_feature = train_states.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "122"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dim = num_stock*num_feature + 1 \\\n",
    "         + num_stock # original states + balance + shares\n",
    "val_steps = len(val_trade) - look_back - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = StockTradingEnv(train_states,\n",
    "                      look_back = look_back, \n",
    "                      feature_num = num_feature,\n",
    "                      steps = 1440,\n",
    "                      valid_env = True,\n",
    "                      balance = balance)\n",
    "env_val = StockTradingEnv(val_states, \n",
    "                          look_back = look_back,\n",
    "                          steps = val_steps,\n",
    "                          feature_num = num_feature,\n",
    "                          balance = balance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obs, _  = env.reset()\n",
    "# obs = [obs]\n",
    "\n",
    "# agent.normalize(obs).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ReplayMemory(replay_size,SEEDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "policy_types = [\"Gaussian\", \"Deterministic\"]\n",
    "policy_type = policy_types[1]\n",
    "\n",
    "writer = SummaryWriter('runs/{}_SAC_{}_{}_{}'.format(datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"), \n",
    "                                                     policy_type,\n",
    "                                                     look_back,\n",
    "                                \"autotune\" if automatic_entropy_tuning and policy_type == \"Gaussian\" else \"\"))\n",
    "num_episodes = 1000\n",
    "agent = SAC(state_dim, \n",
    "            env.action_space,\n",
    "            look_back = look_back,\n",
    "            lr = lr,\n",
    "            automatic_entropy_tuning=automatic_entropy_tuning,\n",
    "            tau = tau,\n",
    "            alpha = alpha,\n",
    "            gamma = gamma,\n",
    "            hidden_size=hidden_size,\n",
    "            policy_type=policy_type,\n",
    "            num_episodes=num_episodes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Training Episode: 1, Avg Episode Reward: -0.000055, Market-Gain: -0.000057, elapase:58.165006s\n",
      "Critic 1 Loss: 3.2402e-06, Critic 2 loss: 2.9905e-06, policy_loss: 0.0006, ent loss: 0.0000\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "Training Episode: 2, Avg Episode Reward: -0.000056, Market-Gain: -0.000057, elapase:58.401585s\n",
      "Critic 1 Loss: 1.4981e-06, Critic 2 loss: 1.5550e-06, policy_loss: 0.0005, ent loss: 0.0000\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "Training Episode: 3, Avg Episode Reward: -0.000055, Market-Gain: -0.000057, elapase:56.911356s\n",
      "Critic 1 Loss: 1.7901e-06, Critic 2 loss: 1.6970e-06, policy_loss: 0.0004, ent loss: 0.0000\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "total_numsteps = 0\n",
    "updates = 0\n",
    "\n",
    "\n",
    "for i_episode in itertools.count(1):\n",
    "    episode_reward = 0\n",
    "    episode_steps = 0\n",
    "    done = False\n",
    "    pre_time = time.time()\n",
    "\n",
    "    state, _ = env.reset()\n",
    "    while not done:\n",
    "        state_ = [state]\n",
    "        action = agent.select_action(state_)  # Sample action from policy\n",
    "\n",
    "        if len(memory) > batch_size:\n",
    "            # Number of updates per step in environment\n",
    "            for i in range(updates_per_step):\n",
    "                # Update parameters of all the networks\n",
    "                critic_1_loss, critic_2_loss, policy_loss, ent_loss, alpha = agent.update_parameters(memory, batch_size, updates)\n",
    "\n",
    "                updates += 1\n",
    "                agent.critic_lr_scheduler.step()\n",
    "                agent.policy_lr_scheduler.step()\n",
    "                \n",
    "\n",
    "        # print(\"updates:\",updates,\"action: \",action[0], 'env steps', env.src.step)\n",
    "        next_state, reward, done, _ = env.step(action) # Step\n",
    "        episode_reward += reward\n",
    "        episode_steps += 1\n",
    "        # important to make a deep copy of the state\n",
    "        memory.push(copy.deepcopy(state), action, reward, next_state, done) # Append transition to memory\n",
    "\n",
    "        state = next_state\n",
    "        \n",
    "    # learning rate scheduling \n",
    "#     agent.critic_lr_scheduler.step()\n",
    "#     agent.policy_lr_scheduler.step()\n",
    "    total_numsteps += 1\n",
    "    \n",
    "    market_gains = np.sum(np.vstack([info['market_gain'] for info in env.infos]),axis=0)[0]\n",
    "    print(\"-------------------------------------------\")\n",
    "    print(\"Training Episode: {:d}, Avg Episode Reward: {:4f}, Market-Gain: {:4f}, elapase:{:4f}s\".format(total_numsteps,\n",
    "                                                                                episode_reward /episode_steps,\n",
    "                                                                                market_gains / episode_steps,\n",
    "                                                                                time.time() - pre_time))\n",
    "    print(\"Critic 1 Loss: {:.4e}, Critic 2 loss: {:.4e}, policy_loss: {:.4f}, ent loss: {:.4f}\".format(critic_1_loss,\n",
    "                                                                                        critic_2_loss,\n",
    "                                                                                        policy_loss,\n",
    "                                                                                        ent_loss))\n",
    "    print(\"-------------------------------------------\")\n",
    "    writer.add_scalar('critic1_loss/train', critic_1_loss, total_numsteps)\n",
    "    writer.add_scalar('critic2_loss/train', critic_2_loss, total_numsteps)\n",
    "    writer.add_scalar('policy_loss/train', policy_loss, total_numsteps)\n",
    "    writer.add_scalar('ent_loss/train', ent_loss, total_numsteps)\n",
    "    if total_numsteps > num_episodes:\n",
    "        break\n",
    "        \n",
    "    \n",
    "    ## validating training every 10 episodoes\n",
    "    if i_episode % 5 == 0:\n",
    "        avg_reward = 0.\n",
    "        episodes = 10\n",
    "        for _  in range(episodes):\n",
    "            state, _ = env_val.reset()\n",
    "            state_ = [state]\n",
    "            episode_reward = 0\n",
    "            done = False\n",
    "            while not done:\n",
    "                action = agent.select_action(state_, evaluate=True)\n",
    "                next_state, reward, done, _ = env_val.step(action)\n",
    "                episode_reward += reward\n",
    "\n",
    "\n",
    "                state = next_state\n",
    "                # print(\"action: \",action[0], 'env steps', env_val.src.step)\n",
    "            avg_reward += episode_reward\n",
    "        avg_reward /= episodes\n",
    "        \n",
    "        writer.add_scalar('avg_reward/test', avg_reward, i_episode)\n",
    "        market_gains = np.sum(np.vstack([info['market_gain'] for info in env_val.infos]),axis=0)[0]\n",
    "        print(\"-------------------------------------------\")\n",
    "        print(\"Testing Episode: {:d}, Episode Reward: {:4f},Market-Gain: {:4f} \".format(i_episode,\n",
    "                                                                                         avg_reward,\n",
    "                                                                                       market_gains))\n",
    "        print(\"-------------------------------------------\")\n",
    "        \n",
    "agent.save_checkpoint(env_name = policy_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianPolicy(\n",
      "  (lstm): LSTM_Module(\n",
      "    (lstm): LSTM(124, 64, num_layers=2)\n",
      "  )\n",
      "  (linear1): Linear(in_features=640, out_features=64, bias=True)\n",
      "  (linear2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (mean_linear1): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (mean_linear2): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (log_std_linear1): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (log_std_linear2): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(agent.policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, _ = env_val.reset()\n",
    "state_ = [state]\n",
    "agent.normalize(state_).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = torch.FloatTensor(np.random.rand(1,30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn1(linear1(input_)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils import clip_grad_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-2.6621e+34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand([1,2])\n",
    "x.view([2,1]).size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
